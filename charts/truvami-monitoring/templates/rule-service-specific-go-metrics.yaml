apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: kube-prometheus-stack-prometheus
    role: alert-rules
  name: truvami-service-specific-go-metrics
spec:
  groups:
    - name: truvami-service-specific-go-metrics
      rules:
        # Bridge Service Specific Alerts (Heavy Kafka processing)
        - alert: TruvamiBridgeGoroutineLeak
          annotations:
            description: >-
              **What's happening:** Truvami Bridge has {{ "{{" }} $value }} goroutines (>2000) - likely goroutine leak in Kafka consumer/producer.
              
              **Why this occurred:** Bridge processes high volumes of Kafka messages and may leak goroutines from:
              - Kafka consumer/producer connection issues
              - Message processing goroutines not properly cleaned up
              - Channel operations blocking indefinitely
              - gRPC client connection goroutines accumulating
              
              **What to do:**
              1. Check Kafka consumer group status and rebalancing
              2. Review gRPC client connection pool health
              3. Monitor message processing queues for backlogs
              4. Check for blocked channels in data forwarding paths
              5. Consider restarting bridge service if leak is severe
            summary: >-
              Truvami Bridge goroutine leak detected ({{ "{{" }} $value }} goroutines)
          expr: go_goroutines{job="truvami-bridge"} > 2000
          for: 3m
          labels:
            namespace: {{ .Release.Namespace }}
            severity: critical
            node: truvami-bridge
            service: truvami-bridge
            component: go-runtime
            {{- with .Values.alertLabels }}
            {{- toYaml . | nindent 12 }}
            {{- end }}

        # API Service Specific Alerts (HTTP server focused)
        - alert: TruvamiAPIGoMemoryLeak
          annotations:
            description: >-
              **What's happening:** Truvami API memory usage growing rapidly - {{ "{{" }} $value | humanizeBytes }} allocated.
              
              **Why this occurred:** API service memory growth often indicates:
              - HTTP request context leaks
              - Database connection pool growth
              - OAuth2 token cache excessive growth
              - Large JSON payloads not being released
              - Request/response buffering issues
              
              **What to do:**
              1. Check HTTP connection pool sizes and limits
              2. Review OAuth2 token cache size and eviction
              3. Monitor database connection pool health
              4. Check for large request/response memory usage
              5. Analyze recent API usage patterns for changes
            summary: >-
              Truvami API memory usage growing rapidly ({{ "{{" }} $value | humanizeBytes }})
          expr: go_memstats_alloc_bytes{job="truvami-api"} > 500 * 1024 * 1024
          for: 5m
          labels:
            namespace: {{ .Release.Namespace }}
            severity: warning
            node: truvami-api
            service: truvami-api
            component: go-runtime
            {{- with .Values.alertLabels }}
            {{- toYaml . | nindent 12 }}
            {{- end }}

        # Siren Service Specific Alerts (Alert processing focused)
        - alert: TruvamiSirenGCPressure
          annotations:
            description: >-
              **What's happening:** Truvami Siren garbage collection running {{ "{{" }} $value }} times/sec - high memory pressure.
              
              **Why this occurred:** Siren processes alerts and may have GC pressure from:
              - High volume of alert objects being created/destroyed
              - Large alert payloads consuming memory
              - Inefficient alert caching strategies
              - Webhook response buffering issues
              - Alert history retention causing memory buildup
              
              **What to do:**
              1. Check alert processing volume and rates
              2. Review alert object lifecycle management
              3. Monitor alert cache sizes and eviction policies
              4. Check webhook response handling and buffering
              5. Analyze alert history retention settings
            summary: >-
              Truvami Siren GC pressure high ({{ "{{" }} $value }}/sec)
          expr: rate(go_gc_duration_seconds_count{job="truvami-siren"}[5m]) > 3
          for: 5m
          labels:
            namespace: {{ .Release.Namespace }}
            severity: warning
            node: truvami-siren
            service: truvami-siren
            component: go-runtime
            {{- with .Values.alertLabels }}
            {{- toYaml . | nindent 12 }}
            {{- end }}

        # Gateway Service Specific Alerts (LoRaWAN processing focused)
        - alert: TruvamiGatewayFileDescriptorLeak
          annotations:
            description: >-
              **What's happening:** Truvami Gateway using {{ "{{" }} $value }} file descriptors - potential connection leak.
              
              **Why this occurred:** Gateway handles many device connections and may leak FDs from:
              - UDP socket connections not properly closed
              - LoRaWAN device session connections accumulating
              - Network interface file descriptors not released
              - Packet capture file handles remaining open
              
              **What to do:**
              1. Check UDP socket connection handling
              2. Review LoRaWAN device session cleanup
              3. Monitor network interface usage patterns
              4. Check packet capture and logging file handling
              5. Verify connection timeout and cleanup settings
            summary: >-
              Truvami Gateway file descriptor usage high ({{ "{{" }} $value }})
          expr: process_open_fds{job="truvami-gateway"} > 200
          for: 5m
          labels:
            namespace: {{ .Release.Namespace }}
            severity: warning
            node: truvami-gateway
            service: truvami-gateway
            component: go-runtime
            {{- with .Values.alertLabels }}
            {{- toYaml . | nindent 12 }}
            {{- end }}

        # Decoder Service Specific Alerts (CPU intensive focused)
        - alert: TruvamiDecoderGoroutineBottleneck
          annotations:
            description: >-
              **What's happening:** Truvami Decoder has {{ "{{" }} $value }} goroutines - potential processing bottleneck.
              
              **Why this occurred:** Decoder processes device payloads and may accumulate goroutines from:
              - Complex payload decoding taking too long
              - Device-specific decoder functions blocking
              - Concurrent decoding operations backing up
              - External decoder service calls timing out
              
              **What to do:**
              1. Check payload decoding duration and complexity
              2. Review decoder function performance and timeouts
              3. Monitor decoder processing queue depths
              4. Check for blocked external decoder service calls
              5. Analyze recent payload types for processing changes
            summary: >-
              Truvami Decoder goroutine bottleneck ({{ "{{" }} $value }} goroutines)
          expr: go_goroutines{job="truvami-decoder"} > 500
          for: 5m
          labels:
            namespace: {{ .Release.Namespace }}
            severity: warning
            node: truvami-decoder
            service: truvami-decoder
            component: go-runtime
            {{- with .Values.alertLabels }}
            {{- toYaml . | nindent 12 }}
            {{- end }}

        # Dashboard Service Specific Alerts (Web server focused)
        - alert: TruvamiDashboardHTTPGoroutineGrowth
          annotations:
            description: >-
              **What's happening:** Truvami Dashboard goroutines growing to {{ "{{" }} $value }} - HTTP handler leak suspected.
              
              **Why this occurred:** Dashboard serves web requests and may leak goroutines from:
              - WebSocket connections not properly closed
              - HTTP handlers not completing properly
              - Long-running database queries blocking handlers
              - SSE (Server-Sent Events) connections accumulating
              - Session management goroutines not cleaned up
              
              **What to do:**
              1. Check WebSocket connection lifecycle management
              2. Review HTTP handler completion and error handling
              3. Monitor database query durations and timeouts
              4. Check SSE connection cleanup and limits
              5. Verify session cleanup and garbage collection
            summary: >-
              Truvami Dashboard HTTP goroutine growth ({{ "{{" }} $value }})
          expr: go_goroutines{job="truvami-dashboard"} > 800
          for: 5m
          labels:
            namespace: {{ .Release.Namespace }}
            severity: warning
            node: truvami-dashboard
            service: truvami-dashboard
            component: go-runtime
            {{- with .Values.alertLabels }}
            {{- toYaml . | nindent 12 }}
            {{- end }}

        # Cross-Service Memory Pressure Alert
        - alert: TruvamiServicesMemoryPressure
          annotations:
            description: >-
              **What's happening:** Multiple Truvami services showing high memory usage simultaneously - system-wide memory pressure.
              
              **Why this occurred:** System-wide memory pressure indicates:
              - Insufficient cluster memory resources
              - Memory leak affecting multiple services
              - High traffic causing memory usage across services
              - Shared dependencies (database, cache) causing memory growth
              
              **What to do:**
              1. **URGENT:** Check cluster memory availability
              2. **Scale up memory resources** if possible
              3. **Identify services with highest memory growth**
              4. **Check shared services** (database, cache, Kafka)
              5. **Consider service restarts** if memory leaks confirmed
            summary: >-
              Multiple Truvami services under memory pressure
          expr: |
            count(
              (go_memstats_alloc_bytes{job=~"truvami-.*"} / go_memstats_sys_bytes{job=~"truvami-.*"}) > 0.8
            ) >= 3
          for: 3m
          labels:
            namespace: {{ .Release.Namespace }}
            severity: critical
            node: truvami-cluster
            service: truvami-cluster
            component: go-runtime
            {{- with .Values.alertLabels }}
            {{- toYaml . | nindent 12 }}
            {{- end }}