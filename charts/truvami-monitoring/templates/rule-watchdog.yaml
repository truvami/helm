{{- if .Values.prometheusRule.enabled }}
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: {{ include "truvami-monitoring.fullname" . }}-watchdog
  namespace: {{ .Release.Namespace | quote }}
  labels:
    {{- include "truvami-monitoring.labels" . | nindent 4 }}
    {{- with .Values.prometheusRule.labels }}
    {{- toYaml . | nindent 4 }}
    {{- end }}
  {{- with .Values.prometheusRule.annotations }}
  annotations:
    {{- toYaml . | nindent 4 }}
  {{- end }}
spec:
  groups:
  - name: truvami.watchdog
    interval: 5m
    rules:
    # Watchdog Alert - Always Firing
    # This alert should always be firing to ensure Prometheus and AlertManager are working
    - alert: TruvamiWatchdog
      expr: vector(1)
      for: 0s
      labels:
        severity: none
        service: monitoring
        component: watchdog
      annotations:
        summary: "Truvami monitoring system watchdog"
        description: |
          This is a watchdog alert that should always be firing.
          If you stop receiving this alert, it means there is an issue with:
          - Prometheus scraping or evaluation
          - AlertManager processing or routing
          - Webhook notification delivery
          
          This alert is used to monitor the health of the monitoring system itself.
        
        {{- if .Values.watchdog.webhookUrl }}
        # Watchdog heartbeat webhook URL
        webhook_url: {{ .Values.watchdog.webhookUrl | quote }}
        {{- end }}
        
    # Dead Man's Switch - Alert if Watchdog stops firing
    - alert: TruvamiWatchdogDeadMansSwitch
      expr: absent_over_time(ALERTS{alertname="TruvamiWatchdog"}[5m])
      for: 1m
      labels:
        severity: critical
        service: monitoring
        component: watchdog
      annotations:
        summary: "Truvami monitoring watchdog has stopped firing"
        description: |
          The TruvamiWatchdog alert has not been firing for the last 5 minutes.
          This indicates a critical issue with the monitoring system:
          
          Possible causes:
          - Prometheus is down or not scraping metrics
          - AlertManager is down or not processing alerts
          - Prometheus rule evaluation is failing
          - Network connectivity issues
          
          Immediate investigation required to restore monitoring capabilities.
        
        runbook_url: "https://github.com/truvami/helm/blob/main/docs/runbooks/monitoring-watchdog.md"

{{- end }}