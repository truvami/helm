{{- if .Values.alerts.deploymentVersions.enabled }}
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: kube-prometheus-stack-prometheus
    role: alert-rules
  name: truvami-deployment-versions
spec:
  groups:
    - name: truvami-deployment-versions
      rules:
        {{- if .Values.alerts.deploymentVersions.multipleVersions.enabled }}
        - alert: TruvamiMultipleVersionsRunning
          annotations:
            description: >-
              **What's happening:** Multiple versions of service {{ "{{" }} $labels.job }} are running simultaneously, indicating a rolling upgrade in progress or deployment issue.

              **Service Details:**
              - Service: {{ "{{" }} $labels.job }}
              - Running Versions: {{ "{{" }} $value }} different versions detected
              - Affected Pods: Check kubectl get pods -l app={{ "{{" }} $labels.job }}

              **Why this occurred:** The system detects version mismatches using count by (job) (count by (job, commit) (truvami_*_info)) > 1
              which counts distinct git commit hashes for each service. Multiple versions running simultaneously can indicate:
              - Normal rolling upgrade in progress (temporary condition)
              - Stuck or failed deployment with old pods not terminating
              - Pod scheduling issues preventing proper rolling update
              - Resource constraints preventing new pods from starting
              - Configuration issues causing deployment rollback
              - Manual intervention required for deployment completion

              **What to do:**
              1. **Check if rolling upgrade is expected** - verify deployment status in Kubernetes
              2. **Monitor upgrade progress** - ensure old pods are terminating properly
              3. **Verify resource availability** - check CPU, memory, and node capacity
              4. **Review deployment logs** for errors or stuck operations
              5. **Check pod status** for failed starts or crashes
              6. **If upgrade stuck >15min:** Consider manual intervention or rollback
              7. **Monitor application health** during version transition
              8. **Alert development team** if unexpected version deployment detected
            summary: >-
              Service {{ "{{" }} $labels.job }} has {{ "{{" }} $value }} versions running - rolling upgrade or deployment issue detected
          expr: >-
            count by (job) (count by (job, commit) ({__name__=~"truvami_(bridge|gateway|siren|api|decoder)_info"})) > 1
          for: {{ .Values.alerts.deploymentVersions.multipleVersions.duration }}
          labels:
            namespace: {{ .Release.Namespace }}
            severity: {{ .Values.alerts.deploymentVersions.multipleVersions.severity }}
            node: truvami-{{ "{{" }} $labels.job }}
            service: truvami-{{ "{{" }} $labels.job }}
            component: version-control
            {{- with .Values.alertLabels }}
            {{- toYaml . | nindent 12 }}
            {{- end }}
        {{- end }}

        {{- if .Values.alerts.deploymentVersions.longRunningMismatch.enabled }}
        - alert: TruvamiLongRunningVersionMismatch
          annotations:
            description: >-
              **What's happening:** Multiple versions of service {{ "{{" }} $labels.job }} have been running for over {{ .Values.alerts.deploymentVersions.longRunningMismatch.duration }} - deployment appears stuck or failed.

              **Service Details:**
              - Service: {{ "{{" }} $labels.job }}
              - Running Versions: {{ "{{" }} $value }} different versions detected
              - Duration: Over {{ .Values.alerts.deploymentVersions.longRunningMismatch.duration }}
              - Check Command: kubectl get deployments,pods -l app={{ "{{" }} $labels.job }}

              **Why this occurred:** The system detects prolonged version mismatches indicating a deployment that hasn't completed properly.
              This is a more serious condition than normal rolling upgrades, suggesting:
              - **Deployment failure:** New pods failing to start or pass health checks
              - **Resource exhaustion:** Insufficient resources to complete rolling upgrade
              - **Configuration errors:** Invalid configuration preventing successful deployment
              - **Image pull failures:** Problems downloading new container images
              - **Pod eviction issues:** Old pods not terminating due to graceful shutdown problems
              - **Manual intervention needed:** Deployment requires admin action to complete

              **What to do IMMEDIATELY:**
              1. **ESCALATE:** This is likely a failed deployment requiring immediate attention
              2. **Check deployment status:** kubectl get deployments,pods -l app={{ "{{" }} $labels.job }}
              3. **Review pod logs:** Look for startup errors or health check failures
              4. **Check resource usage:** Verify sufficient CPU/memory for new pods
              5. **Validate configuration:** Ensure new deployment config is valid
              6. **Consider rollback:** If critical service, consider rolling back to previous version
              7. **Check image availability:** Verify container images are accessible
              8. **Monitor service health:** Ensure service availability during resolution
            summary: >-
              STUCK DEPLOYMENT: Service {{ "{{" }} $labels.job }} has {{ "{{" }} $value }} versions running >{{ .Values.alerts.deploymentVersions.longRunningMismatch.duration }} - immediate action required
          expr: >-
            count by (job) (count by (job, commit) ({__name__=~"truvami_(bridge|gateway|siren|api|decoder)_info"})) > 1
          for: {{ .Values.alerts.deploymentVersions.longRunningMismatch.duration }}
          labels:
            namespace: {{ .Release.Namespace }}
            severity: {{ .Values.alerts.deploymentVersions.longRunningMismatch.severity }}
            node: truvami-{{ "{{" }} $labels.job }}
            service: truvami-{{ "{{" }} $labels.job }}
            component: version-control
            {{- with .Values.alertLabels }}
            {{- toYaml . | nindent 12 }}
            {{- end }}
        {{- end }}

        {{- if .Values.alerts.deploymentVersions.serviceVersionInfo.enabled }}
        - alert: TruvamiServiceVersionInfo
          annotations:
            description: >-
              **Service Version Information:**
              - Service: {{ "{{" }} $labels.job }}
              - Container: {{ "{{" }} $labels.container }}
              - Version: {{ "{{" }} $labels.version }}
              - Commit: {{ "{{" }} $labels.commit }}
              - Build Date: {{ "{{" }} $labels.build_date }}
              - Instance: {{ "{{" }} $labels.instance }}
              - Pod: {{ "{{" }} $labels.pod }}

              This is an informational alert that fires when service version information is available.
              Use this to track deployments and version changes across the environment.
            summary: >-
              Service {{ "{{" }} $labels.job }} running version {{ "{{" }} $labels.version }} (commit: {{ "{{" }} printf "%.8s" $labels.commit }})
          expr: >-
            {__name__=~"truvami_(bridge|gateway|siren|api|decoder)_info"}
          for: {{ .Values.alerts.deploymentVersions.serviceVersionInfo.duration }}
          labels:
            namespace: {{ .Release.Namespace }}
            severity: {{ .Values.alerts.deploymentVersions.serviceVersionInfo.severity }}
            node: truvami-{{ "{{" }} $labels.job }}
            service: truvami-{{ "{{" }} $labels.job }}
            component: version-info
            version: "{{ "{{" }} $labels.version }}"
            commit: "{{ "{{" }} $labels.commit }}"
            {{- with .Values.alertLabels }}
            {{- toYaml . | nindent 12 }}
            {{- end }}
        {{- end }}
{{- end }}
