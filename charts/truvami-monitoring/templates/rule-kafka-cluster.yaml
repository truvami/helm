{{- if .Values.prometheusRule.enabled }}
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: {{ include "truvami-monitoring.fullname" . }}-kafka-cluster
  namespace: {{ .Release.Namespace | quote }}
  labels:
    {{- include "truvami-monitoring.labels" . | nindent 4 }}
    {{- with .Values.prometheusRule.labels }}
    {{- toYaml . | nindent 4 }}
    {{- end }}
  {{- with .Values.prometheusRule.annotations }}
  annotations:
    {{- toYaml . | nindent 4 }}
  {{- end }}
spec:
  groups:
  - name: truvami-kafka-cluster
    interval: {{ .Values.prometheusRule.interval | default "30s" }}
    rules:
    # Kafka Broker Availability
    - alert: TruvamiKafkaBrokerDown
      expr: kafka_brokers < 3
      for: 2m
      labels:
        severity: critical
        service: truvami-kafka
        component: broker
        {{- with .Values.alertLabels }}
        {{- toYaml . | nindent 8 }}
        {{- end }}
      annotations:
        summary: "Kafka broker(s) are down"
        description: |
          Only {{ "{{" }} $value }} out of 3 expected Kafka brokers are available.
          
          This indicates:
          - Broker failure or network issues
          - Potential data loss risk
          - Reduced cluster resilience
          
          Immediate investigation required to restore cluster health.

    # Under-replicated Partitions
    - alert: TruvamiKafkaUnderReplicatedPartitions
      expr: sum(kafka_topic_partition_under_replicated_partition) > 0
      for: 5m
      labels:
        severity: major
        service: truvami-kafka
        component: replication
        {{- with .Values.alertLabels }}
        {{- toYaml . | nindent 8 }}
        {{- end }}
      annotations:
        summary: "Kafka has under-replicated partitions"
        description: |
          {{ "{{" }} $value }} partition(s) are under-replicated.
          
          This indicates:
          - Broker connectivity issues
          - High broker load
          - Potential data durability risk
          
          Check broker health and network connectivity.

    # Consumer Group Lag - Critical
    - alert: TruvamiKafkaConsumerLagCritical
      expr: |
        sum by (consumergroup, topic) (kafka_consumergroup_lag) > 10000
      for: 5m
      labels:
        severity: critical
        service: truvami-kafka
        component: consumer
        node: "{{ "{{" }} $labels.consumergroup }}"
        {{- with .Values.alertLabels }}
        {{- toYaml . | nindent 8 }}
        {{- end }}
      annotations:
        summary: "Critical Kafka consumer lag detected"
        description: |
          Consumer group {{ "{{" }} $labels.consumergroup }} has {{ "{{" }} $value }} messages lag on topic {{ "{{" }} $labels.topic }}.
          
          This indicates:
          - Consumer processing bottleneck
          - Consumer service degradation
          - Potential message processing delays
          
          Immediate action required to prevent data processing delays.

    # Consumer Group Lag - Major
    - alert: TruvamiKafkaConsumerLagHigh
      expr: |
        sum by (consumergroup, topic) (kafka_consumergroup_lag) > 5000
      for: 10m
      labels:
        severity: major
        service: truvami-kafka
        component: consumer
        node: "{{ "{{" }} $labels.consumergroup }}"
        {{- with .Values.alertLabels }}
        {{- toYaml . | nindent 8 }}
        {{- end }}
      annotations:
        summary: "High Kafka consumer lag detected"
        description: |
          Consumer group {{ "{{" }} $labels.consumergroup }} has {{ "{{" }} $value }} messages lag on topic {{ "{{" }} $labels.topic }}.
          
          This may indicate:
          - Increased message volume
          - Consumer processing slowdown
          - Resource constraints
          
          Monitor consumer performance and consider scaling.

    # Topic Partition Leader Issues
    - alert: TruvamiKafkaPartitionLeaderMissing
      expr: sum(kafka_topic_partition_leader == -1) > 0
      for: 2m
      labels:
        severity: critical
        service: truvami-kafka
        component: partition
        {{- with .Values.alertLabels }}
        {{- toYaml . | nindent 8 }}
        {{- end }}
      annotations:
        summary: "Kafka partition(s) without leader"
        description: |
          {{ "{{" }} $value }} partition(s) do not have a leader.
          
          This indicates:
          - Broker election issues
          - Network partitioning
          - Cluster instability
          
          Immediate attention required - topics may be unavailable.

    # Consumer Group Members
    - alert: TruvamiKafkaConsumerGroupEmpty
      expr: kafka_consumergroup_members == 0
      for: 10m
      labels:
        severity: warning
        service: truvami-kafka
        component: consumer
        node: "{{ "{{" }} $labels.consumergroup }}"
        {{- with .Values.alertLabels }}
        {{- toYaml . | nindent 8 }}
        {{- end }}
      annotations:
        summary: "Kafka consumer group has no active members"
        description: |
          Consumer group {{ "{{" }} $labels.consumergroup }} has no active consumer members.
          
          This may indicate:
          - Consumer service downtime
          - Application deployment issues
          - Configuration problems
          
          Verify consumer service health and configuration.

    # High Consumer Group Lag for Critical Topics
    - alert: TruvamiKafkaAlertsTopicLagHigh
      expr: |
        sum by (consumergroup, topic) (kafka_consumergroup_lag{topic=~"alerts\\.(positions|uplinks|battery-statuses|events|rotations)"}) > 1000
      for: 5m
      labels:
        severity: major
        service: truvami-kafka
        component: consumer
        node: "{{ "{{" }} $labels.topic }}"
        {{- with .Values.alertLabels }}
        {{- toYaml . | nindent 8 }}
        {{- end }}
      annotations:
        summary: "High lag on critical alerts topic"
        description: |
          Critical alerts topic {{ "{{" }} $labels.topic }} has {{ "{{" }} $value }} messages lag for consumer group {{ "{{" }} $labels.consumergroup }}.
          
          This affects:
          - Real-time alert processing
          - Position tracking accuracy
          - Device monitoring capabilities
          
          Immediate attention required for alert system functionality.

{{- end }}